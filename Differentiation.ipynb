{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numerical differentiation\n",
    "\n",
    "Suppose we want to apply Newton's method to a function that we know how to evaluate, but don't have code to differentiate.  This is often because it's difficult/error-prone to write or because the interface by which we call it does not support derivatives.  (Commercial packages often fall in this category.)  So we just have a black box function $f(x)$ and wish to approximate its derivative,\n",
    "$$ f'(x) = \\lim_{h\\to 0} \\frac{f(x+h) - f(x)}{h} .$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-3.221101531725168e-06"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def diff(f, x, h=1e-5):\n",
    "    return (f(x + h) - f(x)) / h\n",
    "\n",
    "diff(np.sin, 0.7, 1e-5) - np.cos(0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.77456063177317e-08"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = .5\n",
    "diff(np.tan, x, 1e-8) - 1/np.cos(x)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1e-14, -1271.3873432741966),\n",
       " (1e-12, 140.14843387738802),\n",
       " (1e-10, 0.32726590032689273),\n",
       " (1e-08, 19.79343111347407),\n",
       " (1e-06, 1982.7670766180381),\n",
       " (0.0001, 226466.6387994727)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = 3.14/2\n",
    "[(h, diff(np.tan, x, h) - 1/np.cos(x)**2)\n",
    " for h in [1e-14, 1e-12, 1e-10, 1e-8, 1e-6, 1e-4]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1e-14, -0.0001),\n",
       " (1e-12, -0.0001),\n",
       " (1e-10, -1.1182158029987482e-05),\n",
       " (1e-08, 8.89005823409637e-09),\n",
       " (1e-06, 8.274037095116864e-12),\n",
       " (0.0001, -9.489531298885641e-12),\n",
       " (0.01, -5.0168102921151377e-11)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = 1e4\n",
    "[(h, diff(np.log, x, h) - 1/x)\n",
    " for h in [1e-14, 1e-12, 1e-10, 1e-8, 1e-6, 1e-4, 1e-2]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatically choosing a suitable $h$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1e-14, -1.1191038926094873e-05),\n",
       " (1e-12, -1.109830782817004e-09),\n",
       " (1e-10, -1.109830782817004e-09),\n",
       " (1e-08, -8.599665508239422e-12),\n",
       " (1e-06, -5.0162259288282114e-11),\n",
       " (0.0001, -5.000167712764913e-09),\n",
       " (0.01, -4.967408090441846e-07)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def diff_wp(f, x, eps=1e-8):\n",
    "    \"\"\"Numerical derivative with Walker and Pernice (1998) choice of step\"\"\"\n",
    "    h = eps * (1 + abs(x))\n",
    "    return (f(x+h) - f(x)) / h\n",
    "\n",
    "x = 1e4\n",
    "[(eps, diff_wp(np.log, x, eps) - 1/x) for eps in [1e-14, 1e-12, 1e-10, 1e-8, 1e-6, 1e-4, 1e-2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1e-14, -0.11098307828251563),\n",
       " (1e-12, -0.0008599665507063037),\n",
       " (1e-10, -0.005016225928557105),\n",
       " (1e-08, -0.5000167712751136),\n",
       " (1e-06, -49.67408090441677),\n",
       " (0.0001, -3068.721334766654),\n",
       " (0.01, -9538.52419539635)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = 1e-4\n",
    "[(eps, diff_wp(np.log, x, eps) - 1/x) for eps in [1e-14, 1e-12, 1e-10, 1e-8, 1e-6, 1e-4, 1e-2]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This algorithm is imperfect, leaving some scaling responsibility to the user.\n",
    "It is the default in PETSc's \"matrix-free\" Newton-type solvers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy of numerical differentiation\n",
    "\n",
    "#### Taylor expansion\n",
    "\n",
    "Classical accuracy analysis assumes that functions are sufficiently smooth, meaning that derivatives exist and Taylor expansions are valid within a neighborhood.  In particular,\n",
    "$$ f(x+h) = f(x) + f'(x) h + f''(x) \\frac{h^2}{2!} + \\underbrace{f'''(x) \\frac{h^3}{3!} + \\dotsb}_{O(h^3)} . $$\n",
    "\n",
    "The big-$O$ notation is meant in the limit $h\\to 0$.  Specifically, a function $g(h) \\in O(h^p)$ (sometimes written $g(h) = O(h^p)$) when\n",
    "there exists a constant $C$ such that\n",
    "$$ g(h) \\le C h^p $$\n",
    "for all sufficiently *small* $h$.\n",
    "\n",
    "**Note:** When analyzing algorithms, we will refer to cost being $O(n^2)$, for example, where $n\\to \\infty$.\n",
    "In this case, the above definition applies for sufficiently *large* $n$.  Whether the limit is small ($h\\to 0$) or large ($n \\to\\infty$) should be clear from context.\n",
    "\n",
    "#### Discretization error\n",
    "The `diff` and `diff_wp` functions use a \"forward difference\" formula: $\\tilde f'(x) := (f(x+h) - f(x))/h$.\n",
    "Using the Taylor expansion of $f(x+h)$, we compute the discretization error\n",
    "$$ \\begin{split} \\frac{f(x+h) - f(x)}{h} - f'(x) &= \\frac{f(x) + f'(x) h + f''(x) h^2/2 + O(h^3) - f(x)}{h} - f'(x) \\\\\n",
    "&= \\frac{f'(x) h + f''(x) h^2/2 + O(h^3)}{h} - f'(x) \\\\\n",
    "&= f''(x) h/2 + O(h^2) .\n",
    "\\end{split} $$\n",
    "\n",
    "This is the *discretization error* caused by choosing a finite (not infinitesimal) differencing parameter $h$, and the leading order term depends linearly on $h$.\n",
    "\n",
    "#### Rounding error\n",
    "We have an additional source of error, *rounding error*, which comes from not being able to compute $f(x)$ or $f(x+h)$ exactly, nor subtract them exactly.  Suppose that we can, however, compute these functions with a relative error on the order of $\\epsilon_{\\text{machine}}$.  This leads to\n",
    "$$ \\begin{split}\n",
    "\\tilde f(x) &= f(x)(1 + \\epsilon_1) \\\\\n",
    "\\tilde f(x \\oplus h) &= \\tilde f((x+h)(1 + \\epsilon_2)) \\\\\n",
    "&= f((x + h)(1 + \\epsilon_2))(1 + \\epsilon_3) \\\\\n",
    "&= [f(x+h) + f'(x+h)(x+h)\\epsilon_2 + O(\\epsilon_2^2)](1 + \\epsilon_3) \\\\\n",
    "&= f(x+h)(1 + \\epsilon_3) + f'(x+h)x\\epsilon_2 + O(\\epsilon_{\\text{machine}}^2 + \\epsilon_{\\text{machine}} h)\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "where each $\\epsilon_i$ is an independent relative error on the order of $\\epsilon_{\\text{machine}}$ and we have used a Taylor expansion at $x+h$ to approximate $f(x \\oplus h)$.\n",
    "We thus write the rounding error in the forward difference approximation as\n",
    "$$ \\begin{split}\n",
    "\\left\\lvert \\frac{\\tilde f(x+h) \\ominus \\tilde f(x)}{h} - \\frac{f(x+h) - f(x)}{h} \\right\\rvert &=\n",
    "  \\left\\lvert \\frac{f(x+h)(1 + \\epsilon_3) + f'(x+h)x\\epsilon_2 + O(\\epsilon_{\\text{machine}}^2 + \\epsilon_{\\text{machine}} h) - f(x)(1 + \\epsilon_1) - f(x+h) + f(x)}{h} \\right\\rvert \\\\\n",
    "  &\\le \\frac{|f(x+h)\\epsilon_3| + |f'(x+h)x\\epsilon_2| + |f(x)\\epsilon_1| + O(\\epsilon_{\\text{machine}}^2 + \\epsilon_{\\text{machine}}h)}{h} \\\\\n",
    "  &\\le \\frac{(2 \\max_{[x,x+h]} |f| + \\max_{[x,x+h]} |f' x| \\epsilon_{\\text{machine}} + O(\\epsilon_{\\text{machine}}^2 + \\epsilon_{\\text{machine}} h)}{h} \\\\\n",
    "  &= (2\\max|f| + \\max|f'x|) \\frac{\\epsilon_{\\text{machine}}}{h} + O(\\epsilon_{\\text{machine}}) \\\\\n",
    "\\end{split} $$\n",
    "where we have assumed that $h \\ge \\epsilon_{\\text{machine}}$.\n",
    "This error becomes large (relative to $f'$ -- we are concerned with relative error after all)\n",
    "* $f$ is large compared to $f'$\n",
    "* $x$ is large\n",
    "* $h$ is too small\n",
    "\n",
    "#### Total error and optimal $h$\n",
    "\n",
    "Suppose we would like to choose $h$ to minimize the combined discretization and rounding error,\n",
    "$$ h^* = \\arg\\min_h | f''(x) h/2 | + (2\\max|f| + \\max|f'x|) \\frac{\\epsilon_{\\text{machine}}}{h} $$\n",
    "(dropping the higher order terms), which we can compute by differentiating with respect to $h$ and setting the result equal to zero\n",
    "$$ |f''|/2 - (2\\max|f| + \\max|f'x|) \\frac{\\epsilon_{\\text{machine}}}{h^2} = 0 $$\n",
    "which can be rearranged as\n",
    "$$ h^* = \\sqrt{\\frac{4\\max|f| + 2\\max|f'x|}{|f''|}} \\sqrt{\\epsilon_{\\text{machine}}} .$$\n",
    "Of course this formula is of little use for computing $h$ because all this is to compute $f'$, which we obviously don't know yet, much less $f''$.\n",
    "However, it does have value:\n",
    "* It explains why `1e-8` (i.e., $\\sqrt{\\epsilon_{\\text{machine}}}$) was empirically found to be about optimal for well-behaved/scaled functions.\n",
    "* It explains why even for the best behaved functions, our best attainable accuracy with forward differencing is $\\sqrt{\\epsilon_{\\text{machine}}}$.\n",
    "* If we have some special knowledge about the class of functions we need to differentiate, we might have bounds on these quantities and thus an ability to use this formula to improve accuracy.  Alternatively, we could run a parameter sweep to empirically choose a suitable $h$, though we would have to re-tune in response to parameter changes in the class of functions.\n",
    "* If someone claims to have a simple and robust rule for computing $h$ then this formula tells us how to build a function that breaks their rule.  There are no silver bullets.\n",
    "* If our numerical differentiation routine produces a poor approximation for some function that we run into in the wild, this helps us explain what happened and how to fix it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Centered difference\n",
    "\n",
    "Instead of the forward difference approximation\n",
    "$$ \\frac{f(x+h) - f(x)}{h} $$\n",
    "we could use the centered difference formula,\n",
    "$$ \\frac{f(x+h) - f(x-h)}{2h} . $$\n",
    "(One way to derive this formula is to average a forward and backward difference.  We will learn a more general method later in the course when we do interpolation.)\n",
    "We can compute the discretization error by Taylor expansion,\n",
    "\\begin{split} \\frac{f(x) + f'(x)h + f''(x)h^2/2 + f'''(x)h^3/6 - f(x) + f'(x)h - f''(x)h^2/2 + f'''(x) h^3/6 + O(h^4)}{2h} \\\\\n",
    "= f'(x) + f'''(x)h^2/6 + O(h^3) \\end{split}\n",
    "showing that the leading error term is of order $h^2$, versus order $h$ for forward differences.\n",
    "A similar computation including rounding error will find that the optimal $h$ is now of order $\\sqrt[3]{\\epsilon_{\\text{machine}}}$ so the best attainable accuracy is $\\epsilon_{\\text{machine}}^{2/3}$.\n",
    "This accuracy improvement (versus $\\sqrt{\\epsilon_{\\text{machine}}}$) is significant, but we'll also see that it is twice as expensive when computing derivatives of multi-variate functions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
