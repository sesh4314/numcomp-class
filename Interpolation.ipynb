{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Jupyter notebooks\n",
    "\n",
    "This is a [Jupyter](http://jupyter.org/) notebook using Python.  You can install Jupyter locally to edit and interact with this notebook.\n",
    "\n",
    "# Interpolation and Regression\n",
    "\n",
    "Interpolation and regression address the problem of approximating functions using their (possibly noisy) values at a finite set of points.  There is usually an underlying process from which the observed data are obtained, but this process is impractical to evaluate every time a function value is needed.  Examples of underlying processes include:\n",
    "\n",
    "* direct field observations/measurement of a physical or social system\n",
    "* numerically processed observations, perhaps by applying physical principles\n",
    "* output from an expensive \"exact\" numerical computation\n",
    "* output from an approximate numerical computation\n",
    "\n",
    "We would like an inexpensive deterministic surrogate that we can use instead.  The most common surrogate functions are polynomials and rational functions (ratios of polynomials) because they are convenient to compute with.  Other choices are often made when there is prior knowledge about the behavior of the system, such as using\n",
    "\n",
    "* $\\sin kx$ and $\\cos kx$ to represent periodic functions\n",
    "* powers/exponentials ($a^x$ or $a^{1/x}$) for material properties or reaction rates (e.g., [Arhennius relations](https://en.wikipedia.org/wiki/Arrhenius_equation)).\n",
    "\n",
    "We start our discussion by building surrogate functions that exactly match the observations at a number of points, either given or specially chosen, using polynomials.\n",
    "\n",
    "## Polynomial Interpolation\n",
    "\n",
    "In the Linear Algebra notebook, we discussed Vandermonde matrices which we could use to solve for polynomial coefficients.  It is also possible to compute the coefficients explicitly (rather than by solving a linear system).\n",
    "\n",
    "### Lagrange Interpolating Polynomials\n",
    "\n",
    "Suppose we are given function values $y_0, \\dotsc, y_m$ at the distinct points $x_0, \\dotsc, x_m$ and we would like to build a polynomial of degree $m$ that goes through all these points.  This explicit construction is attributed to Lagrange (though he was not first):\n",
    "\n",
    "$$ p(x) = \\sum_{i=0}^m y_i \\prod_{j \\ne i} \\frac{x - x_j}{x_i - x_j} $$\n",
    "\n",
    "* What is the degree of this polynomial?\n",
    "* Why is $p(x_i) = y_i$?\n",
    "* How expensive (in terms of $m$) is it to evaluate $p(x)$?\n",
    "* How expensive (in terms of $m$) is it to convert to standard form $p(x) = \\sum_{i=0}^m a_i x^i$?\n",
    "* Can we easily evaluate the derivative $p'(x)$?\n",
    "* What can go wrong?  Is this formulation numerically stable?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn')\n",
    "plt.rcParams['figure.max_open_warning'] = False\n",
    "\n",
    "def lagrange(x, y):\n",
    "    @np.vectorize\n",
    "    def p(t):\n",
    "        from numpy import prod\n",
    "        m = len(x) - 1\n",
    "        w = 0\n",
    "        for i in range(m):\n",
    "            w += y[i] * (prod(t - x[:i]) * prod(t - x[i+1:])\n",
    "                / (prod(x[i] - x[:i]) * prod(x[i] - x[i+1:])))\n",
    "        w += y[m] * prod(t - x[:m]) / prod(x[m] - x[:m])\n",
    "        return w\n",
    "    return p\n",
    "\n",
    "x = np.linspace(-1.5,2,4)\n",
    "y = np.sin(x)\n",
    "p = lagrange(x, y)\n",
    "xx = np.linspace(-2,3)\n",
    "\n",
    "plt.plot(x, y, 'o', label='data')\n",
    "plt.plot(xx, p(xx), label='p(x)')\n",
    "plt.plot(xx, np.sin(xx), label='sin(x)')\n",
    "plt.legend(loc='upper left');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Uniqueness\n",
    "\n",
    "Is the polynomial $p(x)$ of degree $m$ that interpolates $m+1$ points unique?  Why?\n",
    "\n",
    "### Vandermonde matrices\n",
    "\n",
    "We have used the Vandermonde matrix with a monomial basis for polynomial interpolation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = np.linalg.solve(np.vander(x), y)\n",
    "plt.plot(x, y, 'o', label='data')\n",
    "plt.plot(xx, np.vander(xx, 4) @ p, label='p(x)')\n",
    "plt.plot(xx, np.sin(xx), label='sin(x)')\n",
    "plt.legend(loc='upper left');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vandermonde matrices are often ill-conditioned and this requires solving an $m\\times m$ linear system, at a cost of $m^3$.\n",
    "\n",
    "### Newton polynomials\n",
    "\n",
    "Newton polynomials are polynomials\n",
    "\n",
    "$$ n_k(x) = \\prod_{i=0}^{k-1} (x - x_i) $$\n",
    "\n",
    "How does the Vandermonde procedure change if we replace $x^k$ with $n_k(x)$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vander_newton(x, abscissa=None):\n",
    "    if abscissa is None:\n",
    "        abscissa = x\n",
    "    n = len(abscissa)\n",
    "    A = np.zeros((len(x), n))\n",
    "    A[:,0] = 1\n",
    "    for i in range(1,n):\n",
    "        A[:,i] = A[:,i-1] * (x - abscissa[i-1])\n",
    "    return A\n",
    "\n",
    "A = vander_newton(np.linspace(-1,1,5))\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Does this affect the cost of solving for the coefficients?\n",
    "* How does the condition number depend on the number and position of the points?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's check that it works.\n",
    "p = np.linalg.solve(vander_newton(x), y)\n",
    "\n",
    "plt.plot(x, y, 'o', label='data')\n",
    "plt.plot(xx, vander_newton(xx, x) @ p, label='p(x)')\n",
    "plt.plot(xx, np.sin(xx), label='sin(x)')\n",
    "plt.legend(loc='upper left');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditioning of the Vandermonde matrix\n",
    "\n",
    "Recall that the condition number of a matrix, the ratio of its largest and smallest singular values\n",
    "$$\\kappa = \\frac{\\sigma_\\max}{\\sigma_\\min} ,$$\n",
    "controls the accuracy achievable using an ideal (backward stable) algorithm to\n",
    "$$ (\\text{error}) \\approx \\kappa \\epsilon_{\\text{machine}} . $$\n",
    "\n",
    "If we will use a Vandermonde matrix to solve interpolation problems, we should check that its condition number is not too big (sometimes we'll say \"bounded\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cond(mat, points, interval=(-1,1), nmax=20):\n",
    "    degree = np.arange(2, nmax)\n",
    "    return degree, np.array(\n",
    "        [np.linalg.cond(mat(points(*interval,n))) \n",
    "         for n in degree])\n",
    "\n",
    "plt.semilogy(*cond(np.vander, np.linspace, (-1,1)), label='monomial')\n",
    "plt.semilogy(*cond(vander_newton, np.linspace, (-1,1)), label='newton')\n",
    "plt.legend(loc='upper left')\n",
    "plt.ylabel('cond(V)')\n",
    "plt.xlabel('$n$');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What if the interval is not centered on the origin?\n",
    "plt.semilogy(*cond(np.vander, np.linspace, (10,12)), label='monomial')\n",
    "plt.semilogy(*cond(vander_newton, np.linspace, (10,12)), label='newton')\n",
    "plt.legend(loc='upper left')\n",
    "plt.ylabel('cond(V)')\n",
    "plt.xlabel('$n$');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions\n",
    "\n",
    "* Vandermonde matrices are typically ill-conditioned. Even with many points, columns typically become nearly linearly dependent.\n",
    "* Interpolation using an arbitrary basis requires $O(n^3)$ operations for $n$ data points because we must solve with a full Vandermonde matrix.\n",
    "* Newton polynomials cause the Vandermonde matrix to be triangular, thus $O(n^2)$ for interpolation.\n",
    "* Newton polynomials can incrementally assimilate new observations: just add extra rows.\n",
    "\n",
    "### Polynomial bases\n",
    "\n",
    "We have seen that monomials and Newton bases are ill-conditioned, but we have a procedure for constructing well-conditioned (orthonormal) bases that span the same space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vander_q(x, n=None, interval=None, print_basis=False):\n",
    "    if n is None:\n",
    "        n = len(x)\n",
    "    if interval is None:\n",
    "        a, b = min(x), max(x)\n",
    "    else:\n",
    "        a, b = interval\n",
    "    # Set up integration on the interval [a,b] using the midpoint rule\n",
    "    w = b - a\n",
    "    V = np.vander(np.linspace(a + 0.5*w/100, b - 0.5*w/100, 100),\n",
    "                     n, increasing=True)\n",
    "    V *= np.sqrt(w/100)\n",
    "    Q, R = np.linalg.qr(V)\n",
    "    if print_basis:\n",
    "        print('R', R)\n",
    "    A = np.vander(x, n, increasing=True)\n",
    "    return np.linalg.solve(R.T, A.T).T\n",
    "\n",
    "p = np.linalg.solve(vander_q(x, print_basis=True), y)\n",
    "\n",
    "plt.plot(x, y, 'o', label='data')\n",
    "plt.plot(xx, vander_q(xx, 4, interval=(min(x), max(x))) @ p, label='p(x)')\n",
    "plt.plot(xx, np.sin(xx), label='sin(x)')\n",
    "plt.legend(loc='upper left');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.semilogy(*cond(np.vander, np.linspace, (-1,1)), label='monomial')\n",
    "plt.semilogy(*cond(vander_newton, np.linspace, (-1,1)), label='newton')\n",
    "plt.semilogy(*cond(vander_q, np.linspace, (-1,1)), label='q')\n",
    "plt.legend(loc='upper left');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosspace(a, b, n=50):\n",
    "    return (a + b)/2 + (b - a)/2 * (\n",
    "        np.cos(np.linspace(-np.pi, 0, n)))\n",
    "\n",
    "plt.semilogy(*cond(np.vander, cosspace, (-1,1)), label='monomial')\n",
    "plt.semilogy(*cond(vander_newton, cosspace, (-1,1)), label='newton')\n",
    "plt.semilogy(*cond(vander_q, cosspace, (-1,1)), label='q')\n",
    "plt.legend(loc='upper left');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "\n",
    "* Orthogonalizing the monomials makes for a much better conditioned basis.\n",
    "* That basis has much smaller condition number for interpolation on equally spaced points.\n",
    "* The condition number still grows exponentially.\n",
    "* Using `cosspace` for interpolation with monomials or Newton basis does not qualitatively change their ill-conditioning.\n",
    "* Using `cosspace` with orthogonal polynomials gives a small condition number.\n",
    "* The orthogonal polynomials can be written as a linear combination of monomials.\n",
    "* That is, a different sequence of constant, linear, quadratic, etc., polynomials.\n",
    "\n",
    "### Another look at these polynomials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-1, 1, 100)\n",
    "M = np.vander(x, 8, increasing=True)\n",
    "N = vander_newton(x, abscissa=np.linspace(-1,1,8))\n",
    "Q = vander_q(x, 8)\n",
    "\n",
    "plt.plot(x, M)\n",
    "plt.title('Monomials')\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(x, N)\n",
    "plt.title('Newton Polynomials')\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(x, Q)\n",
    "plt.title('Orthogonal Polynomials');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constructing these \"good\" (orthogonal) polynomials using QR factorization is a bit cumbersome and depends on a finite accuracy parameter.\n",
    "\n",
    "* Which parameter controls accuracy?\n",
    "\n",
    "### Legendre Polynomials\n",
    "\n",
    "Classical (19th century) mathematics discovered the polynomials we are approximating because they are eigenfunctions (resonant modes) of a differential operator,\n",
    "$$ \\frac{d}{d x} (1 - x^2) \\frac{d P_n(x)}{dx} . $$\n",
    "They can also be derived by exact Gram-Schmidt orthogonalization in the $L^2$ inner product (you did this in Homework 2).\n",
    "Anyway, the classical theory led to a recursive definition\n",
    "$$\\begin{split}\n",
    "P_0(x) &= 1 \\\\\n",
    "P_1(x) &= x \\\\\n",
    "(n+1) P_{n+1}(x) &= (2n+1) x P_n(x) - n P_{n-1}(x)\n",
    "\\end{split}$$\n",
    "\n",
    "We can implement this recurrence in code to efficiently evaluate Legendre polynomials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vander_legendre(x, n=None):\n",
    "    if n is None:\n",
    "        n = len(x)\n",
    "    P = np.ones((len(x), n))\n",
    "    if n > 1:\n",
    "        P[:,1] = x\n",
    "    for k in range(1,n-1):\n",
    "        P[:,k+1] = ((2*k+1) * x * P[:,k] - k * P[:,k-1]) / (k + 1)\n",
    "    return P\n",
    "\n",
    "P = vander_legendre(x, 8)\n",
    "plt.plot(x, P)\n",
    "plt.title('Legendre Polynomials');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Legendre polynomials are well-conditioned\n",
    "\n",
    "plt.semilogy(*cond(vander_q, cosspace, (-1,1)), label='q')\n",
    "plt.semilogy(*cond(vander_legendre, cosspace, (-1,1)), label='legendre')\n",
    "plt.legend(loc='upper left');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chebyshev polynomials\n",
    "\n",
    "There is a related family of polynomials that are even more attractive for interpolation.\n",
    "\n",
    "Define $$ T_n(x) = \\cos (n \\arccos(x)) .$$\n",
    "This turns out to be a polynomial, but it's not obvious why.\n",
    "Recall $$ \\cos(a + b) = \\cos a \\cos b - \\sin a \\sin b .$$\n",
    "Let $y = \\arccos x$ and check\n",
    "$$ \\begin{split}\n",
    "    T_{n+1}(x) &= \\cos \\big( (n+1) y \\big) = \\cos ny \\cos y - \\sin ny \\sin y \\\\\n",
    "    T_{n-1}(x) &= \\cos \\big( (n-1) y \\big) = \\cos ny \\cos y + \\sin ny \\sin y\n",
    "\\end{split}$$\n",
    "Adding these together produces\n",
    "$$ T_{n+1}(x) + T_{n-1}(x) = 2\\cos ny \\cos y = $$\n",
    "which yields a convenient recurrence:\n",
    "$$\\begin{split}\n",
    "T_0(x) &= 1 \\\\\n",
    "T_1(x) &= x \\\\\n",
    "T_{n+1}(x) &= 2 x T_n(x) - T_{n-1}(x)\n",
    "\\end{split}$$\n",
    "which we can also implement in code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vander_chebyshev(x, n=None):\n",
    "    if n is None:\n",
    "        n = len(x)\n",
    "    T = np.ones((len(x), n))\n",
    "    if n > 1:\n",
    "        T[:,1] = x\n",
    "    for k in range(1,n-1):\n",
    "        T[:,k+1] = 2 * x * T[:,k] - T[:,k-1]\n",
    "    return T\n",
    "\n",
    "T = vander_chebyshev(x, 8)\n",
    "plt.plot(x, T)\n",
    "plt.title('Chebyshev Polynomials');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chebyshev polynomials are also well-conditioned\n",
    "\n",
    "plt.semilogy(*cond(vander_q, cosspace, (-1,1)), label='q')\n",
    "plt.semilogy(*cond(vander_legendre, cosspace, (-1,1)), label='legendre')\n",
    "plt.semilogy(*cond(vander_chebyshev, cosspace, (-1,1)), label='chebyshev')\n",
    "plt.legend(loc='upper left');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(cond(vander_chebyshev, cosspace, (-1,1))[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is actually amazing: converting from the values at some special points to the coefficients of some specially crafted polynomials has a constant condition number of about 1.6.  As we will find later, Chebyshev interpolation has a number of other remarkable properties.\n",
    "\n",
    "## Runge Effect\n",
    "\n",
    "We've seen before that the accuracy of an interpolating polynomial is often poor near (and beyond) the ends of the interval.  We've also found \"good\" bases for representing the polynomials and found that when \"good\" points are used, the condition number can be small independent of polynomial order/number of points.  When points are poorly spaced, however, the condition number grows rapidly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cond(vander_chebyshev, np.linspace, (-1,1))[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chebyshev_interp_and_eval(x, xx):\n",
    "    \"\"\"Matrix mapping from values at points x to values\n",
    "    of Chebyshev interpolating polynomial at points xx\"\"\"\n",
    "    A = vander_chebyshev(x)\n",
    "    B = vander_chebyshev(xx, len(x))\n",
    "    return B @ np.linalg.inv(A)\n",
    "\n",
    "print(np.linalg.cond(chebyshev_interp_and_eval(cosspace(-1,1,20),\n",
    "                                               np.linspace(-1,1,1000))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.linalg.cond(chebyshev_interp_and_eval(cosspace(-1,1,20),\n",
    "                                               np.linspace(-1,1,100))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.linalg.cond(chebyshev_interp_and_eval(np.linspace(-1,1,20),\n",
    "                                               np.linspace(-1,1,100))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.linalg.cond(chebyshev_interp_and_eval(cosspace(-1,1,20),\n",
    "                                               np.linspace(-2,2,100))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What are we seeing?\n",
    "\n",
    "* Constructing the polynomials from `x=cosspace` points is good (well conditioned)\n",
    "* Constructing the polynomial from `x=linspace` points is bad (ill conditioned)\n",
    "* We can evaluate anywhere within the interval `(-1,1)` accurately; it doesn't matter whether we use `linspace` or `cosspace`.\n",
    "* Evaluating outside the interval (\"extrapolation\") is terrible\n",
    "\n",
    "### What does this ill-conditioning look like in practice?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runge1(x):\n",
    "    return 1 / (1 + 10*x**2)\n",
    "x = np.linspace(-1,1,20)\n",
    "xx = np.linspace(-1,1,100)\n",
    "\n",
    "plt.plot(x, runge1(x), '*')\n",
    "plt.plot(xx, chebyshev_interp_and_eval(x, xx) @ runge1(x));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-1,1,21)\n",
    "\n",
    "plt.plot(x, runge1(x), '*')\n",
    "plt.plot(xx, chebyshev_interp_and_eval(x, xx) @ runge1(x));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runge2(x):\n",
    "    return np.exp(-(4*x)**2)\n",
    "\n",
    "plt.plot(x, runge2(x), '*')\n",
    "plt.plot(xx, chebyshev_interp_and_eval(x, xx) @ runge2(x));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x = cosspace(-1,1,20)\n",
    "\n",
    "plt.plot(x, runge1(x), '*')\n",
    "plt.plot(x, runge2(x), '^')\n",
    "plt.plot(xx, chebyshev_interp_and_eval(x, xx) @ runge1(x))\n",
    "plt.plot(xx, chebyshev_interp_and_eval(x, xx) @ runge2(x));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def runge3(x):\n",
    "    return 1.*(x > 0)\n",
    "\n",
    "plt.plot(x, runge3(x), '*')\n",
    "plt.plot(xx, chebyshev_interp_and_eval(x, xx) @ runge3(x));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions\n",
    "\n",
    "* The condition number is the ratio of largest singular value ($\\sigma_{\\max} = \\lVert A \\rVert$) to the smallest $\\sigma_{\\min}$.  Is the condition number large because the norm is large or because the smallest is tiny?\n",
    "* Does the condition number of the `interp_and_eval` procedure above depend on the basis used to represent polynomials?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x = np.linspace(-1,1,20)\n",
    "A = chebyshev_interp_and_eval(x, xx)\n",
    "print(A.shape, np.linalg.cond(A))\n",
    "U, S, V = np.linalg.svd(A, full_matrices=0)\n",
    "print(U.shape, S.shape, V.shape)\n",
    "print(S[:4])\n",
    "\n",
    "plt.plot(xx, U[:,:2])\n",
    "plt.plot(x, V.T[:,:2], 'o')\n",
    "plt.title('Worst amplification');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lagrange interpolating polynomials revisited\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-1, 1, 9)\n",
    "Alin = chebyshev_interp_and_eval(x, xx)\n",
    "plt.plot(xx, Alin[:,0:5])\n",
    "plt.plot(x[:5], 0*x[:5]+1, 'ok')\n",
    "plt.plot(x, 0*x, 'sk')\n",
    "plt.title('linspace');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = cosspace(-1, 1, 9)\n",
    "Acos = chebyshev_interp_and_eval(x, xx)\n",
    "plt.plot(xx, Acos[:,0:5])\n",
    "plt.plot(x[:5], 0*x[:5]+1, 'ok')\n",
    "plt.plot(x, 0*x, 'sk')\n",
    "plt.title('cosspace');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy\n",
    "\n",
    "Up to this point, we have been primarily concerned with **stability**.  That is, we have been looking for formulations in which small changes to the input do not produce large changes to the output.  Intrinsically, this problem \"should\" have a norm of about 1 (with suitable scaling, or using the max ($\\infty$) norm -- changing the input function should change the output function by the same amount.\n",
    "\n",
    "Now we explore accuracy: the dependence of error on the cost of interpolation.  We'll start with piecewise constant interpolation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_nearest(x, xx):\n",
    "    \"\"\"For each target (xx), find the index of the nearest source point (x)\"\"\"\n",
    "    i = x.argsort()  # Indices that sort x\n",
    "    x = x[i]         # x sorted\n",
    "    loc = x.searchsorted(xx)\n",
    "    loc -= abs(xx - x[loc-1]) < abs(xx - x.take(loc, mode='wrap'))\n",
    "    return i[loc]\n",
    "\n",
    "find_nearest(np.array([0, -1, 1]), [-.3, -.9, .2, .7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def piecewise_constant_interp_and_eval(x, xx):\n",
    "    \"\"\"Construct matrix for interpolation of data at x, evaluating at xx\"\"\"\n",
    "    nearest = find_nearest(x, xx)\n",
    "    A = np.zeros((len(xx), len(x)))\n",
    "    A[np.arange(len(xx)), nearest] = 1\n",
    "    return A\n",
    "\n",
    "x = np.linspace(-1,1,15)\n",
    "xx = np.linspace(-1, 1, 100)\n",
    "\n",
    "plt.ylim(-0.2, 1.2)\n",
    "plt.plot(x, runge3(x), '*')\n",
    "plt.plot(xx, piecewise_constant_interp_and_eval(x, xx) @ runge3(x), label='interp')\n",
    "plt.plot(xx, runge3(xx), label='exact')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-1,1,20)\n",
    "plt.plot(x, runge1(x), '*')\n",
    "plt.plot(xx, piecewise_constant_interp_and_eval(x, xx) @ runge1(x), label='interp')\n",
    "plt.plot(xx, runge1(xx), label='exact')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.linalg.cond(piecewise_constant_interp_and_eval(\n",
    "    np.linspace(-1,1,71),\n",
    "    np.linspace(-1,1,100))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maxerror(interp_and_eval, f, xspace, interval, npoints):\n",
    "    error = []\n",
    "    for n in npoints:\n",
    "        x = xspace(*interval, n)\n",
    "        xx = np.linspace(*interval, 300)\n",
    "        A = interp_and_eval(x, xx)\n",
    "        error.append(np.linalg.norm(A @ f(x) - f(xx), np.inf))\n",
    "    return error\n",
    "\n",
    "npoints = np.arange(2, 30)\n",
    "\n",
    "plt.loglog(npoints, maxerror(piecewise_constant_interp_and_eval, \n",
    "                             runge1, np.linspace, (-1,1), npoints),\n",
    "              label='piecewise_constant')\n",
    "plt.loglog(npoints, maxerror(chebyshev_interp_and_eval, \n",
    "                             runge1, cosspace, (-1,1), npoints), \n",
    "              label='chebyshev cos')\n",
    "plt.loglog(npoints, maxerror(chebyshev_interp_and_eval, \n",
    "                             runge1, np.linspace, (-1,1), npoints), \n",
    "              label='chebyshev lin')\n",
    "plt.loglog(npoints, 1/npoints, label='slope=-1')\n",
    "plt.loglog(npoints, npoints**(-2.), label='slope=-2')\n",
    "plt.legend(loc='lower left')\n",
    "plt.xlabel('Number of points')\n",
    "plt.ylabel('Max error');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.semilogy(npoints, maxerror(piecewise_constant_interp_and_eval,\n",
    "                               runge1, np.linspace, (-1,1), npoints), \n",
    "                label='piecewise_constant')\n",
    "plt.semilogy(npoints, maxerror(chebyshev_interp_and_eval, \n",
    "                               runge1, cosspace, (-1,1), npoints), \n",
    "                label='chebyshev cos')\n",
    "plt.semilogy(npoints, maxerror(chebyshev_interp_and_eval, \n",
    "                               runge1, np.linspace, (-1,1), npoints), \n",
    "                label='chebyshev lin')\n",
    "plt.semilogy(npoints, 1/npoints, label='$n^{-1}$')\n",
    "plt.semilogy(npoints, npoints**(-2.), label='$n^{-2}$')\n",
    "plt.legend(loc='lower left')\n",
    "plt.xlabel('Number of points')\n",
    "plt.ylabel('Max error');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations\n",
    "\n",
    "* Piecewise constant interpolation is **stable** on any set of points\n",
    "* Piecewise constant interpolation **converges very slowly** (needs many points to increase accuracy)\n",
    "* Chebyshev/polynomial interpolation **requires special input points**, otherwise it is **not stable**\n",
    "* Chebyshev/polynomial interpolation has **\"exponential\" convergence**\n",
    "\n",
    "## Splines\n",
    "\n",
    "If we are given an arbitrary distribution of points, interpolation with a single polynomial is not robust.  Piecewise constant interpolation is not very accurate and gives a rough function.\n",
    "We could improve the accuracy by using a piecewise linear function (see homework 2-Interpolation), but the accuracy is still limited and the function still isn't smooth (there is a \"corner\" where the slope changes at each data point).\n",
    "Splines are a way to guarantee an arbitrary amount of smoothness.\n",
    "The idea is that given sorted input points $\\{x_i\\}_{i=0}^n$, we compute an interpolating polynomial $s_i(x)$ on every interval $(x_i, x_{i+1})$.\n",
    "\n",
    "#### Interpolation\n",
    "Given a function value $y_i$ at each $x_i$, we require\n",
    "$$\\begin{split}\n",
    "  s_i(x_i) &= y_i \\\\\n",
    "  s_i(x_{i+1}) &= y_{i+1}\n",
    "  \\end{split} $$\n",
    "so that the polynomial interpolates our data.\n",
    "If the polynomial has order greater than 1, we are left with some extra degrees of freedom.\n",
    "To provide a unique solution, we'll need to add conditions.\n",
    "\n",
    "#### Smoothness\n",
    "\n",
    "The conditions above guarantee continuity, but not smoothness.  We use our extra degree(s) of freedom to impose smoothness conditions of the form\n",
    "$$\\begin{split}\n",
    "  s_i'(x_{i+1}) &= s_{i+1}'(x_{i+1}) \\\\\n",
    "  s_i''(x_{i+1}) &= s_{i+1}''(x_{i+1}) .\n",
    "\\end{split}$$\n",
    "These conditions, which are applied at the interior nodes ($x=1,\\dotsc,n-1$) couple the splines from adjacent intervals and causes the spline approximation to be globally coupled.\n",
    "\n",
    "#### End-point conditions\n",
    "\n",
    "The conditions above are still not enough to guarantee a unique spline.\n",
    "Suppose we use quadratic polynomials for each $s_i$.  Then with $n$ intervals, we have $n$ degrees of freedom after imposing the interpolation condition.  Meanwhile, there are only $n-1$ internal nodes.  If we impose continuity of the first derivative, we have $n - (n-1) = 1$ undetermined degrees of freedom.  We could fix this by imposing a boundary condition, such as that the slope at one end-point (e.g., $s_0'(x_0)$) was equal to a known value.  This is not symmetric and is often an unnatural condition.\n",
    "\n",
    "Suppose we use cubic polynomials.  Now we have two degrees of freedom per interval after imposing the interpolation condition.  If we impose continuity of the first and second derivatives, we have $2n - 2(n-1) = 2$ remaining degrees of freedom.  A common choice here is the \"natural spline\", $s_0''(x_0) = 0$ and $s_n''(x_n) = 0$.  Cubic splines are the most popular spline in this family.\n",
    "\n",
    "### Solving spline interpolation problems\n",
    "\n",
    "We need to choose a basis for the polynomials $s_i(x)$.  We could choose\n",
    "$$ s_i(x) = a_i + b_i x + c_i x^2 + d_i x^3 $$\n",
    "but this would be very ill-conditioned when the interval $(x_i,x_{i+1})$ is far from zero.\n",
    "A better-conditioned choice is\n",
    "$$ s_i(x) = a_i + b_i(x - x_i) + c_i(x - x_i)^2 + d_i(x - x_i)^3 . $$\n",
    "The interpolation property gives\n",
    "$$\\begin{split}\n",
    "a_i &= y_i \\\\\n",
    "a_i + b_i(x_{i+1} - x_i) + c_i(x_{i+1} - x_i)^2 + d_i(x_{i+1} - x_i)^3 &= y_{i+1}\n",
    "\\end{split}$$\n",
    "and continuity of the first and second derivatives gives\n",
    "$$\\begin{split}\n",
    "  b_i + 2c_i(x_{i+1} - x_i) + 3d_i(x_{i+1}-x_i)^2 &= b_{i+1} \\\\\n",
    "  2c_i + 6d_i(x_{i+1} - x_i) &= 2 c_{i+1} .\n",
    "\\end{split}$$\n",
    "After trivially eliminating the $a_i$, this is a block bidiagonal system ($3\\times 3$ blocks).  We can reduce this to a scalar tridiagonal system.  Define $\\delta_i = x_{i+1} - x_i$ and $\\Delta_i = y_{i+1} - y_i$.  Then eliminate $d_i$ using\n",
    "$$ d_i = \\frac{c_{i+1} - c_i}{3\\delta_i} $$\n",
    "and $b_i$ using\n",
    "$$\\begin{split}\n",
    "  b_i\\delta i &= \\Delta_i - c_i\\delta_i^2 - \\underbrace{\\frac{c_{i+1} - c_i}{3\\delta_i}}_{d_i} \\delta_i^3 \\\\\n",
    "  b_i &= \\frac{\\Delta_i}{\\delta_i} - \\frac{\\delta_i}{3}(c_{i+1} + 2c_i) .\n",
    "\\end{split}$$\n",
    "Substituting into the equation for continuity of the first derivative gives\n",
    "$$ \\frac{\\Delta_i}{\\delta i} - \\frac{\\delta_i}{3}(c_{i+1} + 2c_i) + 2c_i\\delta_i + (c_{i+1} - c_i)\\delta_i = \n",
    "\\frac{\\Delta_{i+1}}{\\delta_{i+1}} - \\frac{\\delta_{i+1}}{3}(c_{i+2} + 2c_{i+1}) $$\n",
    "which reduces to\n",
    "$$ \\delta_i c_i + 2(\\delta_i + \\delta_{i+1}) c_{i+1} + \\delta_{i+1} c_{i+2} = 3\\left(\\frac{\\Delta_{i+1}}{\\delta_{i+1}} - \\frac{\\Delta_i}{\\delta_i}\\right) . $$\n",
    "To impose boundary conditions, we add a dummy interval on the right end (the actual value of $x_{n+1}>x_n$ cancels out) so that the equation above is valid for $i = 0,\\dotsc,n-2$ and the right boundary condition $s_{n-1}''(x_n) = s_n''(x_n)$ becomes $c_n = 0$. The left boundary condition $s_0''(x_0) = 0$ yields $c_0 = 0$, so we must solve\n",
    "$$\\begin{bmatrix}\n",
    "1 & & & & & & \\\\\n",
    "\\delta_0 & 2(\\delta_0+\\delta_1) & \\delta_1 & & & & \\\\\n",
    "& \\delta_1 & 2(\\delta_1+\\delta_2) & \\delta_2 & & & \\\\\n",
    "& & \\ddots & \\ddots & \\ddots & & \\\\\n",
    "& & & & \\delta_{n-2} & 2(\\delta_{n-2}+\\delta_{n-1}) & \\delta_{n-1} \\\\\n",
    "& & & & & & 1 \\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix} c_0 \\\\ c_1 \\\\ \\vdots \\\\ \\\\ c_{n-1} \\\\ c_n \\end{bmatrix} = \n",
    "\\begin{bmatrix} 0 \\\\ 3\\left(\\frac{\\Delta_1}{\\delta_1} - \\frac{\\Delta_0}{\\delta_0} \\right) \\\\ \\vdots \\\\ 3\\left(\\frac{\\Delta_{n-1}}{\\delta_{n-1}} - \\frac{\\Delta_{n-2}}{\\delta_{n-2}}\\right) \\\\ 0 \\end{bmatrix} .$$\n",
    "After solving this equation for $c_i$, we will recover $b_i$ and $d_i$, and then can evaluate the spline at arbitrary points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spline_cubic_interp_and_eval(x, xx):\n",
    "    n = len(x) - 1\n",
    "    def s_interp(y):\n",
    "        perm = np.argsort(x)\n",
    "        xs = x[perm]\n",
    "        ys = y[perm]\n",
    "        delta = xs[1:] - xs[:-1]\n",
    "        Delta = ys[1:] - ys[:-1]\n",
    "        \n",
    "        # Assemble tridiagonal system (this can be optimized)\n",
    "        T = np.zeros((n+1, n+1))\n",
    "        T[0,0] = 1\n",
    "        for i in range(1,n):\n",
    "            T[i,i-1:i+2] = [delta[i-1], 2*(delta[i-1]+delta[i]), delta[i]]\n",
    "        T[-1,-1] = 1\n",
    "        rhs = np.zeros(n+1)\n",
    "        rhs[1:-1] = 3*(Delta[1:]/delta[1:] - Delta[:-1]/delta[:-1])\n",
    "        c = np.linalg.solve(T, rhs)\n",
    "        S = np.zeros((n, 5))    # Matrix to hold splines as [xs,d,c,b,a]\n",
    "        S[:,0] = xs[:-1]                                  # Sorted interval left ends\n",
    "        S[:,2] = c[:-1]                                   # From tridiagonal solve\n",
    "        S[:,4] = ys[:-1]                                  # Interpolation property\n",
    "        S[:,1] = (c[1:] - c[:-1])/(3*delta)               # Recover d\n",
    "        S[:,3] = Delta/delta - delta/3*(2*c[:-1] + c[1:]) # Recover b\n",
    "        return S\n",
    "    def s_eval(S, xx):\n",
    "        left = S[:,0].searchsorted(xx) - 1\n",
    "        left[left<0] = 0 # Use the leftmost interval even if xx<=x\n",
    "        f = np.zeros_like(xx)\n",
    "        for i,t in enumerate(xx):\n",
    "            f[i] = np.polyval(S[left[i],1:], xx[i] - S[left[i],0])\n",
    "        return f\n",
    "    \n",
    "    # Build an explicit matrix for the spline fit evaluated at xx\n",
    "    A = np.zeros((len(xx), len(x)))\n",
    "    for i,e in enumerate(np.eye(len(x), len(x))):\n",
    "        S = s_interp(e)\n",
    "        A[:,i] = s_eval(S, xx)\n",
    "    return A\n",
    "\n",
    "spline_cubic_interp_and_eval(np.linspace(-1,1,3),\n",
    "                             np.linspace(-1,1,9))\n",
    "plt.plot(spline_cubic_interp_and_eval(np.linspace(-1,1,10),\n",
    "                                      np.linspace(-1,1,100))[:,:5]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-1,1,10)\n",
    "xx = np.linspace(-1, 1, 100)\n",
    "plt.ylim(-0.2, 1.2)\n",
    "plt.plot(x, runge1(x), '*')\n",
    "plt.plot(xx, spline_cubic_interp_and_eval(x, xx) @ runge1(x), label='spline')\n",
    "plt.plot(xx, runge1(xx), label='exact')\n",
    "plt.legend(loc='upper left');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the accuracy of cubic splines for smooth equations\n",
    "\n",
    "npoints = np.arange(2,30)\n",
    "\n",
    "plt.loglog(npoints, maxerror(piecewise_constant_interp_and_eval,\n",
    "                             runge1, np.linspace, (-1,1), npoints),\n",
    "              label='piecewise_constant')\n",
    "plt.loglog(npoints, maxerror(chebyshev_interp_and_eval, \n",
    "                             runge1, cosspace, (-1,1), npoints), \n",
    "              label='chebyshev cos')\n",
    "plt.loglog(npoints, maxerror(spline_cubic_interp_and_eval, \n",
    "                             runge1, np.linspace, (-1,1), npoints), \n",
    "              label='spline_cubic')\n",
    "plt.loglog(npoints, 1/npoints, label='$n^{-1}$')\n",
    "plt.loglog(npoints, 10*npoints**(-4.), label='$n^{-4}$')\n",
    "plt.legend(loc='lower left')\n",
    "plt.xlabel('Number of points')\n",
    "plt.ylabel('Max error');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.semilogy(npoints, maxerror(piecewise_constant_interp_and_eval, \n",
    "                               runge1, np.linspace, (-1,1), npoints),\n",
    "                label='piecewise_constant')\n",
    "plt.semilogy(npoints, maxerror(chebyshev_interp_and_eval, \n",
    "                               runge1, cosspace, (-1,1), npoints), \n",
    "                label='chebyshev cos')\n",
    "plt.semilogy(npoints, maxerror(spline_cubic_interp_and_eval, \n",
    "                               runge1, np.linspace, (-1,1), npoints), \n",
    "                label='spline_cubic')\n",
    "plt.semilogy(npoints, 1/npoints, label='$n^{-1}$')\n",
    "plt.semilogy(npoints, npoints**(-2.), label='$n^{-2}$')\n",
    "plt.legend(loc='lower left')\n",
    "plt.xlabel('Number of points')\n",
    "plt.ylabel('Max error');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations\n",
    "\n",
    "* Splines are smooth by construction\n",
    "* Splines can have artifacts at sharp transitions\n",
    "* Fitting with splines requires solving a tridiagonal system, $O(n)$ cost\n",
    "* Moving, adding, or removing any control point $x_i$ requires rebuilding the tridiagonal matrix (not a local operation)\n",
    "* Splines appear to be reasonably accurate, at least for smooth functions\n",
    "* Evaluating splines requires finding which interval in which to evaluate\n",
    "* Storing splines appears to require more data than the input function (store $n\\times 5$ matrix `S`), but everything can be locally reconstructed from the points $x_i$ and coefficients $c_i$ -- thus requiring $2n$ storage for general $x_i$ or just $n$ if $x_i$ is not arbitrarily spaced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Are splines well conditioned?\n",
    "\n",
    "print(np.linalg.cond(\n",
    "        spline_cubic_interp_and_eval(np.linspace(-1,1,40),\n",
    "                                     np.linspace(-1,1,500))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splines can represent functions with local features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return np.sin(1 / (.1 + x**2))\n",
    "\n",
    "n = 12\n",
    "xl = np.linspace(-1, 1, n)\n",
    "x = .5*(xl + xl**3)\n",
    "xx = np.linspace(-1, 1, 100)\n",
    "plt.plot(x, f(x), 'o')\n",
    "plt.plot(xx, spline_cubic_interp_and_eval(x, xx) @ f(x), label='spline')\n",
    "xc = cosspace(-1, 1, n)\n",
    "plt.plot(xc, f(xc), 's')\n",
    "plt.plot(xx, chebyshev_interp_and_eval(xc, xx) @ f(xc), label='chebyshev')\n",
    "plt.plot(xx, f(xx), label='exact')\n",
    "plt.legend(loc='upper left');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Outlook\n",
    "* The artifacts at sharp transitions can be relieved by relaxing continuity.  For example, splines can be broken at any point by replacing derivative continuity with endpoint conditions.  This also decouples the tridiagonal system.  Such methods produce [Bezier curves](https://en.wikipedia.org/wiki/B%C3%A9zier_curve), which are used for many applications including fonts.\n",
    "* [B-splines](https://en.wikipedia.org/wiki/B-spline) are a more computationally convenient representation of a piecewise polynomial curve, but are not inherently *interpolatory*.  They are *local* in the sense that modifying control points has only local effects.\n",
    "* [NURBS](https://en.wikipedia.org/wiki/Non-uniform_rational_B-spline) (Non-Uniform Rational B-Splines) are an extension of B-splines to use rational functions.  This allows exact representation of conic sections.  NURBS are used heavily for Computer Aided Design (CAD) and other engineering applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple dimensions\n",
    "\n",
    "Suppose that we wish to approximate functions of multiple variables.  For example, we may wish to construct a scalar-valued function $f(\\mathbf x)$ where $\\mathbf x \\in \\mathbb R^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ricker(x, s=.25):\n",
    "    return (1 - (x/s)**2) * np.exp(-.5*(x/s)**2)\n",
    "\n",
    "xlin = np.linspace(-1, 1)\n",
    "plt.plot(xlin, ricker(xlin));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sombrero(x, y, s=.25):\n",
    "    r = np.sqrt(x**2 + y**2)\n",
    "    return ricker(r, s)\n",
    "\n",
    "xx, yy = np.meshgrid(xlin, xlin)\n",
    "plt.contourf(xx, yy, sombrero(xx, yy))\n",
    "plt.colorbar();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "axes = plt.subplot(projection='3d')\n",
    "axes.plot_surface(xx, yy, sombrero(xx, yy), cmap=plt.get_cmap());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A generalized Vandermonde matrix can be used here as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.kron([[1,2],[3,4]], [[1, 10],[100,1000]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vander_chebyshev2d(x0, x1, n0=None, n1=None):\n",
    "    \"\"\"Construct a generalized Vandermonde matrix using a tensor product of\n",
    "    Chebyshev polynomials.\n",
    "    \"\"\"\n",
    "    V0 = vander_chebyshev(x0, n0)\n",
    "    V1 = vander_chebyshev(x1, n1)\n",
    "    return np.kron(V0, V1)\n",
    "\n",
    "n = 9\n",
    "xcos = cosspace(-1, 1, n)\n",
    "x, y = np.meshgrid(xcos, xcos)\n",
    "z = sombrero(x, y)\n",
    "V = vander_chebyshev2d(xcos, xcos)\n",
    "c = np.linalg.solve(V, z.flatten())\n",
    "zz = vander_chebyshev2d(xlin, xlin, n, n) @ c\n",
    "zz = zz.reshape(xx.shape)\n",
    "\n",
    "axes = plt.subplot(111, projection='3d')\n",
    "axes.scatter(x, y, z, 'o', color='k')\n",
    "axes.plot_surface(xx, yy, zz, cmap=plt.get_cmap())\n",
    "axes.set_title('Chebyshev approximation: {}*{} points'.format(n, n))\n",
    "\n",
    "error = zz - sombrero(xx, yy)\n",
    "plt.figure()\n",
    "axerr = plt.subplot(111, projection='3d')\n",
    "axerr.plot_surface(xx, yy, error, cmap=plt.get_cmap())\n",
    "axerr.set_title('Error {}'.format(np.linalg.norm(error.flatten(), np.inf)));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations\n",
    "\n",
    "* $n^2$ samples of the function were needed to build this approximation.\n",
    "* The size of the Vandermonde matrix is $n^2 \\times n^2$, thus costs $O(n^6)$ to factor and $O(n^4)$ to store (naively).\n",
    "* Smart algorithms can take advantage of the [Kronecker product](https://en.wikipedia.org/wiki/Kronecker_product) to solve in $O(n^3)$ time and $O(n^2)$ space.\n",
    "* For the specific case of Chebyshev polynomials and `cosspace` points, the transformation can be done in $O(n^2 \\log n)$ time and $O(n^2)$ space.\n",
    "\n",
    "### Curse of dimensionality\n",
    "\n",
    "If it takes $n$ points to approximate a function of one variable, it takes $n^d$ points to approximate a similarly rich function of $d$ variables.\n",
    "\n",
    "* Naive multivariate polynomial interpolation builds an $n^d\\times n^d$ matrix, thus costs $O(n^{3d})$ time to factor and $O(n^{2d})$ to store.\n",
    "* The Kronecker product reduces this to $O(n^{d+1})$ time to solve and $O(n^d)$ storage.\n",
    "* The need for $O(n^d)$ interpolation points does not go away for general functions.\n",
    "* Some functions exhibit *decay of mixed derivatives* and can be represented using [Sparse grids](https://en.wikipedia.org/wiki/Sparse_grid) with only $O(n \\log^{d-1} n)$ points.\n",
    "\n",
    "#### Smart algorithms are critical for approximation in many dimensions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
