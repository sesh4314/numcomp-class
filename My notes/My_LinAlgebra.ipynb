{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Algebra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The matrix-vector product $y = A x$ is a linear combination of the columns of $A$.  The familiar definition,\n",
    "\n",
    "$$ y_i = \\sum_j A_{i,j} x_j $$\n",
    "\n",
    "can also be viewed as\n",
    "\n",
    "$$ y = \\Bigg[ A_{:,0} \\Bigg| A_{:,1} \\Bigg| \\dotsm \\Bigg] \\begin{bmatrix} x_0 \\\\ x_1 \\\\ \\vdots \\end{bmatrix}\n",
    "= \\Bigg[ A_{:,0} \\Bigg] x_0 + \\Bigg[ A_{:,1} \\Bigg] x_1 + \\dotsb . $$\n",
    "\n",
    "The notation $A_{i,j}$ corresponds to the Python syntax `A[i,j]` and the colon `:` means the entire range (row or column).  So $A_{:,j}$ is the $j$th column and $A_{i,:}$ is the $i$th row.  The corresponding Python syntax is `A[:,j]` and `A[i,:]`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Code implementation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matmul1: [ 50 130 290]\n",
      "matmul2: [ 50 130 290]\n",
      "matmul3: [ 50 130 290]\n"
     ]
    }
   ],
   "source": [
    "def matmult1(A, x):\n",
    "    \"\"\"Entries of y are dot products of rows of A with x\"\"\"\n",
    "    y = np.zeros_like(A[:,0])\n",
    "    for i in range(len(A)):\n",
    "        row = A[i,:]\n",
    "        for j in range(len(row)):\n",
    "            y[i] += row[j] * x[j]\n",
    "    return y\n",
    "\n",
    "def matmult2(A, x):\n",
    "    \"\"\"Same idea, but more compactly\"\"\"\n",
    "    y = np.zeros_like(A[:,0])\n",
    "    for i,row in enumerate(A):\n",
    "        y[i] = row.dot(x)\n",
    "    return y\n",
    "\n",
    "def matmult3(A, x):\n",
    "    \"\"\"y is a linear expansion of the columns of A\"\"\"\n",
    "    y = np.zeros_like(A[:,0])\n",
    "    for j,col in enumerate(A.T):\n",
    "        y += col * x[j]\n",
    "    return y\n",
    "\n",
    "A = np.array([[1,2],[3,5],[7,11]])\n",
    "x = np.array([10,20])\n",
    "\n",
    "print(\"matmul1: {}\".format(matmult1(A, x)))\n",
    "print(\"matmul2: {}\".format(matmult2(A, x)))\n",
    "print(\"matmul3: {}\".format(matmult3(A, x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 50 130 290]\n",
      "[ 50 130 290]\n"
     ]
    }
   ],
   "source": [
    "#or we can just use Python's\n",
    "print(A.dot(x))\n",
    "print(A@x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some common terminology\n",
    "\n",
    "* The **range** of $A$ is the space spanned by its columns.  This definition coincides with the range of a function $f(x)$ when $f(x) = A x$.\n",
    "* The (right) **nullspace** of $A$ is the space of vectors $x$ such that $A x = 0$.\n",
    "* The **rank** of $A$ is the dimension of its range.\n",
    "* A matrix has **full rank** if the nullspace of either $A$ or $A^T$ is empty (only the 0 vector).  Equivalently, if all the columns of $A$ (or $A^T$) are linearly independent.\n",
    "* A **nonsingular** (or **invertible**) matrix is a square matrix of full rank.  We call the inverse $A^{-1}$ and it satisfies $A^{-1} A = A A^{-1} = I$.\n",
    "\n",
    "$\\DeclareMathOperator{\\rank}{rank} \\DeclareMathOperator{\\null}{null} $\n",
    "If $A \\in \\mathbb{R}^{m\\times m}$, which of these doesn't belong?\n",
    "1. $A$ has an inverse $A^{-1}$\n",
    "2. $\\rank (A) = m$\n",
    "3. $\\null(A) = \\{0\\}$\n",
    "4. $A A^T = A^T A$\n",
    "5. $\\det(A) \\ne 0$\n",
    "6. $A x = 0$ implies that $x = 0$\n",
    "\n",
    "Answer: 4, code below proves this\n",
    "\n",
    "When we write $x = A^{-1} y$, we mean that $x$ is the unique vector such that $A x = y$.\n",
    "(It is rare that we explicitly compute a matrix $A^{-1}$, though [it's not as \"bad\"](https://arxiv.org/abs/1201.6035) as people may have told you.)\n",
    "A vector $y$ is equivalent to $\\sum_i e_i y_i$ where $e_i$ are columns of the identity.\n",
    "Meanwhile, $x = A^{-1} y$ means that we are expressing that same vector $y$ in the basis of the columns of $A$, i.e., $\\sum_i A_{:,i} x_i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2 3]\n",
      " [0 4]]\n",
      "[[13 12]\n",
      " [12 16]]\n",
      "[[ 4  6]\n",
      " [ 6 25]]\n"
     ]
    }
   ],
   "source": [
    "B = np.array([[2, 3],[0, 4]])\n",
    "print(B)\n",
    "print(B @ B.T)\n",
    "print(B.T @ B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vandermonde Matrices\n",
    "\n",
    "A Vandermonde matrix is a matrix whose columns are functions evaluated at discrete points.\n",
    "\n",
    "i.e. their column elements are functions.\n",
    "\n",
    "Given a few points, we can use the vandermonde matrix to find the polynomial through those points.\n",
    "\n",
    "This is because every function can be represented as a polynomial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Orthogonal matrices\n",
    "\n",
    "- If $x^T y = 0$ then we say $x$ and $y$ are **orthogonal** (or \"$x$ is orthogonal to $y$\").\n",
    "- A vector is said to be **normalized** if $\\lVert x \\rVert = 1$.\n",
    "- If $x$ is orthogonal to $y$ and $\\lVert x \\rVert = \\lVert y \\rVert = 1$ then we say $x$ and $y$ are **orthonormal**.\n",
    "- A square matrix with orthonormal columns is said to be an **orthogonal matrix**.\n",
    "\n",
    "We typically use $Q$ or $U$ and $V$ for matrices that are known/constructed to be orthogonal.\n",
    "- Orthogonal matrices are always full rank -- the columns are linearly independent.\n",
    "- The inverse of a orthogonal matrix is its transpose:\n",
    "$$ Q^T Q = Q Q^T = I . $$\n",
    "Orthogonal matrices are a powerful building block for robust numerical algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gram-Schmidt Orthogonalization\n",
    "\n",
    "Given a collection of vectors (columns of a matrix), we can find an orthogonal basis by applying the above procedure one column at a time and saving the result.  Let's think of the first two columns,\n",
    "$$ \\Bigg[ a_0 \\, \\Bigg| \\, a_1 \\Bigg] = \\Bigg[ q_0 \\,\\Bigg|\\, q_1 \\Bigg]\n",
    "\\begin{bmatrix} r_{00} & r_{01} \\\\ 0 & r_{11} \\end{bmatrix} . $$\n",
    "#### Column 0\n",
    "The equation for column 0 reads\n",
    "$$ a_0 = q_0 r_{00} $$\n",
    "and we require that $\\lVert q_0 \\rVert = 1$, thus\n",
    "$$ r_{00} = \\lVert a_0 \\rVert $$\n",
    "and\n",
    "$$ q_0 = a_0 / r_{00} . $$\n",
    "#### Column 1\n",
    "This equation reads\n",
    "$$ a_1 = q_0 r_{01} + q_1 r_{11} $$\n",
    "where $a_1$ and $q_0$ are known and we will require that $q_0^T q_1 = 0$.\n",
    "We can find the part of $a_1$ that is orthogonal to $q_0$ via\n",
    "$$ (I - q_0 q_0^T) a_1 = a_1 - q_0 \\underbrace{q_0^T a_1}_{r_{01}} $$\n",
    "leaving a sub-problem equivalent to that of column 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Theorem: all full-rank $m\\times n$ matrices ($m \\ge n$) have a unique $Q R$ factorization with $R_{j,j} > 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solving equations using QR\n",
    "\n",
    "To solve\n",
    "$$ A x = b $$\n",
    "we can compute $A = QR$ and then\n",
    "$$ x = R^{-1} Q^T b . $$\n",
    "\n",
    "This also works for non-square systems!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Check out the code at this section for an example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"Right-looking algorithms\"\n",
    "\n",
    "The implementations above have been \"left-looking\"; when working on column $i$, we compare it only to columns to the left (i.e., $j < i$).  We can reorder the algorithm to look to the right by projecting $q_i$ out of all columns $j > i$.  This algorithm is stable while being just as parallel as `gram_schmidt_classical`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Check out the code at this section for an example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Condition number of a matrix\n",
    "\n",
    "We may have informally referred to a matrix as \"ill-conditioned\" when the columns are nearly linearly dependent, but let's make this concept for precise.  Recall the definition of (relative) condition number from the Rootfinding notes,\n",
    "\n",
    "$$ \\kappa = \\max_{\\delta x} \\frac{|\\delta f|/|f|}{|\\delta x|/|x|} . $$\n",
    "\n",
    "We understood this definition for scalar problems, but it also makes sense when the inputs and/or outputs are vectors (or matrices, etc.) and absolute value is replaced by vector (or matrix) norms.  Let's consider the case of matrix-vector multiplication, for which $f(x) = A x$.\n",
    "\n",
    "$$ \\kappa(A) = \\max_{\\delta x} \\frac{\\lVert A (x+\\delta x) - A x \\rVert/\\lVert A x \\rVert}{\\lVert \\delta x\\rVert/\\lVert x \\rVert}\n",
    "= \\max_{\\delta x} \\frac{\\lVert A \\delta x \\rVert}{\\lVert \\delta x \\rVert} \\, \\frac{\\lVert x \\rVert}{\\lVert A x \\rVert} = \\lVert A \\rVert \\frac{\\lVert x \\rVert}{\\lVert A x \\rVert} . $$\n",
    "\n",
    "There are two problems here:\n",
    "\n",
    "* I wrote $\\kappa(A)$ but my formula depends on $x$.\n",
    "* What is that $\\lVert A \\rVert$ beastie?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least squares and the normal equations\n",
    "\n",
    "A **least squares problem** takes the form: given an $m\\times n$ matrix $A$ ($m \\ge n$), find $x$ such that\n",
    "$$ \\lVert Ax - b \\rVert $$\n",
    "is minimized.  If $A$ is square and full rank, then this minimizer will satisfy $A x - b = 0$, but that is not the case in general because $b$ is not in the range of $A$.\n",
    "The residual $A x - b$ must be orthogonal to the range of $A$.\n",
    "\n",
    "* Is this the same as saying $A^T (A x - b) = 0$?\n",
    "* If $QR = A$, is it the same as $Q^T (A x - b) = 0$?\n",
    "\n",
    "In the quiz, we showed that $QQ^T$ is an orthogonal projector onto the range of $Q$.  If $QR = A$,\n",
    "$$ QQ^T (A x - b) = QQ^T(Q R x - b) = Q (Q^T Q) R x - QQ^T b = QR x - QQ^T b = A x - QQ^T b . $$\n",
    "So if $b$ is in the range of $A$, we can solve $A x = b$.  If not, we need only *orthogonally* project $b$ into the range of $A$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
