{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rootfinding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking for root\n",
    "\n",
    "This function checks if $f(x)$ has a root in the given range $[a,b]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hasroot(f, a, b, xrange = 1000):\n",
    "    ''' Within the range, the min/max values will have opposite\n",
    "    signs if the func crosses y=0\n",
    "    '''\n",
    "    x = np.linspace(a,b,1000)\n",
    "    y = f(x)\n",
    "    \n",
    "    return np.min(y)*np.max(y) < 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bisection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Algorithm:**\n",
    "\n",
    "1. Start with an initial interval $[a,b]$ such that $f(a)$ and $f(b)$ have opposite signs.\n",
    "2. Propose a new point $c$, which is the midpoint of the interval: $c = (a+b)/2$.\n",
    "3. Create a new interval by replacing either $a$ or $b$ with $c$, such that the function still has opposite sign on the two ends of the interval.\n",
    "4. Loop back to step 2 and repeat until the size of the interval is smaller than the tolerance $\\epsilon$.\n",
    "5. Return the midpoint of the final interval as the result.\n",
    "\n",
    "Step 3 is the tricky one here; how do we choose which point to replace?  When we take the midpoint between $a$ and $b$, we are guaranteed that __one of three things will happen:__\n",
    "\n",
    "1. $f(a)$ and $f(c)$ will have opposite sign, so we replace $b$ with $c$;\n",
    "2. $f(b)$ and $f(c)$ will have opposite sign, so we replace $a$ with $c$;\n",
    "3. $f(c)$ is exactly zero, in which case we're done: $c$ is the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bisect(f, a, b, history=None):\n",
    "    mid = (a + b)/2.\n",
    "    if b-a < 1e-5:\n",
    "        if history is None:\n",
    "            return mid\n",
    "        else:\n",
    "            return mid, np.array(history)\n",
    "    if history is not None:\n",
    "        history.append(mid)\n",
    "    if hasroot(f, a, mid):\n",
    "        return bisect(f, a, mid, history)\n",
    "    else:\n",
    "        return bisect(f, mid, b, history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f0(x):\n",
    "    return x**2 - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.4142112731933594"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#no history\n",
    "bisect(f0, -2, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.4142112731933594,\n",
       " array([1.        , 1.5       , 1.25      , 1.375     , 1.4375    ,\n",
       "        1.40625   , 1.421875  , 1.4140625 , 1.41796875, 1.41601562,\n",
       "        1.41503906, 1.41455078, 1.41430664, 1.41418457, 1.41424561,\n",
       "        1.41421509, 1.41419983, 1.41420746]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#w/ history\n",
    "bisect(f0, 0,2,history=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Newton's Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method:\n",
    "\n",
    "$$x_{i+1} = x_i - \\frac{f(x_i)}{f'(x_i)}$$\n",
    "\n",
    "until $x_{i+1} = 0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def newton(func, x, verbose=False):\n",
    "    \"\"\"Solve f(x) = 0 using initial guess x.\n",
    "    \n",
    "    The provided function func must return a pair of values,\n",
    "    f(x) and its derivative f'(x).  For example, to solve\n",
    "    the equation x^2 - 3 starting from initial guess x=1,\n",
    "    one would write\n",
    "    \n",
    "    def func(x):\n",
    "        return x**2 - 3, 2*x\n",
    "        \n",
    "    newton(func, 1)\n",
    "    \"\"\"\n",
    "    for i in range(100):\n",
    "        fx, dfx = func(x)\n",
    "        if verbose:\n",
    "            print(func.__name__, i, x, fx)\n",
    "        if np.abs(fx) < 1e-12:\n",
    "            return x, fx, i\n",
    "        try:\n",
    "            x -= fx / dfx\n",
    "        except ZeroDivisionError:\n",
    "            return x, np.NaN, i\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Secant Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method:\n",
    "\n",
    "$$\n",
    "x_{n} = x_{n-1} - f(x_{n-1})\\frac{x_{n-1} - x_{n-2}}{f(x_{n-1}) - f(x_{n-1})}\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\frac{x_{n-2}f(x_{n-1}) - x_{n-1}f(x_{n-1})}{f(x_{n-1}) - f(x_{n-1})}\n",
    "$$\n",
    "\n",
    "until $x_n = 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A rootfinding algorithm produces a sequence of approximations $x_i$, s.t.\n",
    "\n",
    "$$\\lim_{i \\to \\infty} x_i \\to x_*$$\n",
    "where $f(x_*) = 0$, For analysis, it is convenient to define the errors $e_i = x_i - x_*$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We say that an iterative alg is **$q$-linearly convergent** ($q$ for quotient) if\n",
    "\n",
    "$$\\lim_{i \\to \\infty} \\frac{|e_{i+1}|}{|e_i|} = \\rho < 1.$$\n",
    "\n",
    "where $\\rho$ is the convergence factor. Small $\\rho$ represents faster convergence.\n",
    "\n",
    "So, this is satisfied when error gets smaller with each iteration $i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On a weaker condition, we say that an iterative alg is **$r$-linearly convergent** or it goes through **linear convergence** if\n",
    "\n",
    "$$|e_i| \\le \\epsilon_i$$\n",
    "\n",
    "for all sufficiently large $i$ when the sequence $\\{\\epsilon_i\\}$ converges $q$-linearly to 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixed Point Iteration\n",
    "\n",
    "Consider the iteration $$x_{i+1} = g(x_i)$$\n",
    "\n",
    "s'pose there was a fixed point $r = g(r)$. By the *mean value theorum*, we have that \n",
    "\n",
    "$$ x_{i+1} - r = g(x_i) - g(r) = g'(c_i) (x_i - r) $$\n",
    "\n",
    "for some $c_i$ between $x_i$ and $r$.\n",
    "\n",
    "Taking absolute values, $$|e_{i+1}| = |g'(c_i)| |e_i|,$$ \n",
    "which converges to zero if $|g'(c_i)| < 1$.\n",
    "\n",
    "If $|g'(r)| < 1$ then for any $\\epsilon > 0$ there is a neighborhood of $r$ such that \n",
    "\n",
    "$$|g'(c_i)| < |g'(r)| + \\epsilon$$ for \n",
    "all $c_i$ in that neighborhood (guaranteed any time $x_i$ is in the neighborhood)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Theorem: Linear Convergence of Fixed Point Iteration\n",
    "\n",
    "If $g$ is continuously differentiable, $r = g(r)$, and $|g'(r)| < 1$ then the fixed point iteration $x_{i+1} = g(x_i)$ is locally linearly convergent with rate $|g'(r)|$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Newton's method has **locally quadratic convergence** to simple roots:\n",
    "\n",
    "$$\\lim_{i \\to \\infty} \\frac{|e_{i+1}|}{|e_i|^2} < \\infty.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditioning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Floating point arithmetic\n",
    "Floating point arithmetic $x \\circledast y := \\text{float}(x * y)$ is exact within a relative accuracy $\\epsilon_{\\text{machine}}$.  Formally,\n",
    "$$ x \\circledast y = (x * y) (1 + \\epsilon) $$\n",
    "for some $|\\epsilon| \\le \\epsilon_{\\text{machine}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Absolute Condition Number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a function $f: X \\to Y$ and define the *absolute condition number*\n",
    "$$ \\hat\\kappa = \\lim_{\\delta \\to 0} \\max_{|\\delta x| < \\delta} \\frac{|f(x + \\delta x) - f(x)|}{|\\delta x|} = \\max_{\\delta x} \\frac{|\\delta f|}{|\\delta x|}. $$\n",
    "If $f$ is differentiable, then $\\hat\\kappa = |f'(x)|$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relative condition number\n",
    "\n",
    "Given the relative nature of floating point arithmetic, it is more useful to discuss **relative condition number**,\n",
    "$$ \\kappa = \\max_{\\delta x} \\frac{|\\delta f|/|f|}{|\\delta x|/|x|}\n",
    "= \\max_{\\delta x} \\Big[ \\frac{|\\delta f|/|\\delta x|}{|f| / |x|} \\Big] $$\n",
    "or, if $f$ is differentiable,\n",
    "$$ \\kappa = \\max_{\\delta x} |f'(x)| \\frac{|x|}{|f|} . $$\n",
    "\n",
    "How does a condition number get big?\n",
    "\n",
    "#### Take-home message\n",
    "\n",
    "The relative accuracy of the best-case algorithm will not be reliably better than $\\epsilon_{\\text{machine}}$ times the condition number.\n",
    "$$ \\max_{\\delta x} \\frac{|\\delta f|}{|f|} \\ge \\kappa \\cdot \\epsilon_{\\text{machine}} .$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stability\n",
    "\n",
    "We use the notation $\\tilde f(x)$ to mean a numerical algorithm for approximating $f(x)$.  Additionally, $\\tilde x = x (1 + \\epsilon)$ is some \"good\" approximation of the exact input $x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Forward) Stability\n",
    "**\"nearly the right answer to nearly the right question\"**\n",
    "$$ \\frac{\\lvert \\tilde f(x) - f(\\tilde x) \\rvert}{| f(\\tilde x) |} \\in O(\\epsilon_{\\text{machine}}) $$\n",
    "for some $\\tilde x$ that is close to $x$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward Stability\n",
    "**\"exactly the right answer to nearly the right question\"**\n",
    "$$ \\tilde f(x) = f(\\tilde x) $$\n",
    "for some $\\tilde x$ that is close to $x$\n",
    "\n",
    "* Every backward stable algorithm is stable.\n",
    "* Not every stable algorithm is backward stable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy of backward stable algorithms (Theorem)\n",
    "\n",
    "A backward stable algorithm for computing $f(x)$ has relative accuracy\n",
    "$$ \\left\\lvert \\frac{\\tilde f(x) - f(x)}{f(x)} \\right\\rvert \\in O(\\kappa(f) \\epsilon_{\\text{machine}}) . $$\n",
    "This is a rewording of a statement made earlier -- backward stability is the best case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
